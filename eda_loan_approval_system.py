# -*- coding: utf-8 -*-
"""EDA_Loan Approval System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sv6QW4TJX0C9sCKhxevV5ocqAyY1sxbx

# Problem Statement
## Business Use-case
- Home loan company/bank gets application for loan from variour customers (loan applicants) where based on certain inputs company validates the customer eligibility for loan. 
- We want to automate the loan eligibility process (real time) based on customer detail obtained during loan application. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. 
## Business goal
- If the loan approval process is automated, it can save a lot of man hours and improve the speed of service to the customers. 
- The increase in customer satisfaction and savings in operational costs are significant. 
- However, the benefits can only be reaped if the bank has a robust model to accurately predict which application it should approve and which to reject, in order to minimize the risk of loan default.

## Translating Problem into Data Science / Machine Learning use case
- This is a classification problem where we have to predict whether a loan will be approved or not. 
- Specifically, it is a binary classification problem where we have to predict either one of the two classes given i.e. approved (Y) or not approved (N).  
- Other way to frame the problem is to predict whether the loan will likely to default or not. 
- The dependent variable is the Loan_Status, while the rest are independent variable or features. 
- We need to develop a model using the features to predict the target variable.
"""

# Commented out IPython magic to ensure Python compatibility.
# import libraries
# %matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)
import warnings
warnings.filterwarnings("ignore")

from google.colab import files
uploaded = files.upload()

"""## Exploratory Data Analysis (EDA)"""

# Import data 
df = pd.read_csv("loan_train.csv")
print('Information on dataset:')
df.info()

print('Head:'); print(df.head()); print(); print('Tail:'); print(df.tail());

"""
## Hypotheses 
Through this process, all the possible factors are listed here which can affect the outcome i.e. which of the features will have an impact on whether a loan will be approved or not. 

Some of the hypothesis are:
- Education - Applicants with higher education level i.e. graduate level should have higher chances of loan approval
- Income: Applicants with higher income should have more chances of loan approval
- Loan amount: If the loan amount is less, the chances of loan approval should be high
- Loan term: Loans with shorter time period should have higher chances of approval
- Previous credit history: Applicants who have repayed their previous debts should have higher chances of loan approval
- Monthly installment amount: If the monthly installment amount is low, the chances of loan approval should be high And so on

Some of the hypothesis are intuitive while others may not. We will try to validate each of these hypothesis based on the dataset."""

df.describe()

"""# Exploratory data analysis

## Univariate analysis

###Target Variable (Categorical)
"""

plt.style.use('ggplot')

from pylab import rcParams
import matplotlib.ticker as mtick # For specifying the axes tick format 
rcParams['figure.figsize']=10,6

# bar plot to visualize the frequency
ax = (df['Loan_Status'].value_counts()*100.0 /len(df)).plot(kind='bar', stacked = True, rot = 0)
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.set_ylabel('% Loan application')
ax.set_xlabel('Status')
ax.set_ylabel('% Customers')
ax.set_title('Application Distribution')

totals = []  # creating a list to collect the plt.patches data

# finding the values and append to list
for i in ax.patches:
    totals.append(i.get_width())

total = sum(totals)  # setting individual bar lables using above list

for i in ax.patches:
    # getting_width pulls left or right; get_y pushes up or down
    ax.text(i.get_x()+.15, i.get_height()-3.5, \
            str(round((i.get_height()/total), 1))+'%', color='white', weight = 'bold')

"""The loan of around 68.7% was approved. There is no imbalanced classes issue in this dataset, thus accuracy as an evaluation metric should be appropriate.

### Independent Variable (Categorical)
There are 5 features that are categorical or binary (Gender, Married, Self_Employed, Credit_History, Education)
"""

# Visualizing categorical features
# plt.figure(1)
plt.subplot(231)
df['Gender'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender')

plt.subplot(232)
df['Married'].value_counts(normalize=True).plot.bar(title= 'Married')

plt.subplot(233)
df['Self_Employed'].value_counts(normalize=True).plot.bar(title= 'Self_Employed')

plt.subplot(234)
df['Credit_History'].value_counts(normalize=True).plot.bar(title= 'Credit_History')

plt.subplot(235)
df['Education'].value_counts(normalize=True).plot.bar(title= 'Education')

plt.show()

"""- 80% applicants in the dataset are male.
- 65% of the applicants in the dataset are married.
- 15% applicants in the dataset are self employed.
- 85% applicants have credit history (repaid their debts).
- 80% of the applicants are Graduate.
### Independent Variable (Ordinal)
There are 2 features that are Ordinal: Variables in categorical features having some order involved (Dependents, Property_Area)
"""

# Remaining categorical features
plt.subplot(121)
df['Dependents'].value_counts(normalize=True).plot.bar(figsize=(10,4), title= 'Dependents')

plt.subplot(122)
df['Property_Area'].value_counts(normalize=True).plot.bar(title= 'Property_Area')

plt.show()

"""- More than half of the applicants don’t have any dependents.
- Most of the applicants are from Semiurban area.
### Independent Variable 
There are 4 features that are Numerical: 
- ApplicantIncome, 
- CoapplicantIncome, 
- LoanAmount, 
- Loan_Amount_Term
"""

numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
print(numerical_columns)

fig,axes = plt.subplots(1,3,figsize=(17,5))
for idx,cat_col in enumerate(numerical_columns):
    sns.boxplot(y=cat_col,data=df,x='Loan_Status',ax=axes[idx])

print(df[numerical_columns].describe())
plt.subplots_adjust(hspace=1)
plt.tight_layout()

"""### Applicant income distribution"""

plt.subplot(121)
sns.distplot(df['ApplicantIncome']);

plt.subplot(122)
df['ApplicantIncome'].plot.box(figsize=(10,5))

plt.show()

"""- Most of the data in the distribution of applicant income is towards left which means it is not normally distributed. 
- Distribution is right-skewed. 
- We need to make it normal.

The boxplot confirms the presence of a lot of outliers/extreme values. 
"""

# Income vs Education
df.boxplot(column='ApplicantIncome', by = 'Education')
plt.suptitle(""); plt.show()

"""higher number of graduates with very high incomes, which are appearing to be the outliers."""

# co-applicant income distribution
plt.subplot(121)
sns.distplot(df['CoapplicantIncome']);

plt.subplot(122)
df['CoapplicantIncome'].plot.box(figsize=(10,5))
plt.show()

"""Majority of coapplicant’s income ranges from 0 to 5000. We also see a lot of outliers in the coapplicant income and it is not normally distributed."""

# distribution of LoanAmount
plt.subplot(121)
sns.distplot(df['LoanAmount']);

plt.subplot(122)
df['LoanAmount'].plot.box(figsize=(10,5))

plt.show()

"""normal distribution but still slightly right-skewed for LoanAmount but there are lot of outliers in this variable. """

# distribution of Loan_Amount_Term
df['Loan_Amount_Term'].value_counts(normalize=True).plot.bar(title= 'Loan_Amount_Term')
plt.show()

"""- 85% of the loans are 360 months term/30 years period
## Bivariate Analysis

### Categorical Independent Variable vs Target Variable

"""

print(pd.crosstab(df['Gender'],df['Loan_Status']))
Gender = pd.crosstab(df['Gender'],df['Loan_Status'])
Gender.div(Gender.sum(1).astype(float), axis = 0).plot(kind="bar", stacked=True, figsize=(6,4))
plt.xlabel('Gender')
p = plt.ylabel('Percentage')
plt.show()

print(pd.crosstab(df['Married'],df['Loan_Status']))
Married = pd.crosstab(df['Married'],df['Loan_Status'])
Married.div(Married.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(6,4))
plt.xlabel('Married')
p = plt.ylabel('Percentage')

print(pd.crosstab(df['Dependents'],df['Loan_Status']))
Dependents=pd.crosstab(df['Dependents'],df['Loan_Status'])
Dependents.div(Dependents.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('Dependents')
p = plt.ylabel('Percentage')

print(pd.crosstab(df['Education'],df['Loan_Status']))
Education=pd.crosstab(df['Education'],df['Loan_Status'])
Education.div(Education.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(6,4))
plt.xlabel('Education')
p = plt.ylabel('Percentage')

print(pd.crosstab(df['Self_Employed'], df['Loan_Status']))
Self_Employed=pd.crosstab(df['Self_Employed'],df['Loan_Status'])
Self_Employed.div(Self_Employed.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(6,4))
plt.xlabel('Self_Employed')
p = plt.ylabel('Percentage')

print(pd.crosstab(df['Credit_History'], df['Loan_Status']))
Credit_History=pd.crosstab(df['Credit_History'], df['Loan_Status'])
Credit_History.div(Credit_History.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(6,4))
plt.xlabel('Credit_History')
p = plt.ylabel('Percentage')

print(pd.crosstab(df['Property_Area'],df['Loan_Status']))
Property_Area=pd.crosstab(df['Property_Area'],df['Loan_Status'])
Property_Area.div(Property_Area.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('Property_Area')
P = plt.ylabel('Percentage')

"""- proportion of male and female applicants is more or less same for both approved and unapproved loans
- proportion of married applicants is higher for the approved loans
distribution of applicants with 1 or 3+ dependents is similar across both the categories of Loan_Status there is nothing significant we can infer from Self_Employed vs Loan_Status plot.
- proportion of loans getting approved for graduates is higher compared to non-graduates it seems people with credit history as 1 are more likely to get their loans approved
- proportion of loans getting approved in semiurban area is higher as compared to that in rural or urban areas.
### Numerical Independent Variable vs Target Variable
"""

df[df['ApplicantIncome'] > 20000].sort_values(by = 'ApplicantIncome')

"""All applicants with an income greater than 20,000 have higher level of education, it seems reasonable that these applicants would have higher income. However, since most of the dataset is comprised of applicants with higher education this alone would not explain the difference. However, for most of these points there is nothing indicating we should remove these points.

If we look at row 409, we see that this applicant's income is the largest in our dataset, and suspiciously ends in three 0's. Futhermore, the property area is rural, the credit history is marked 0, and the loan status is marked as having been declined. Given this information it is most likely that the applicant income was entered incorrectly. 

**We should drop this point**
"""

df = df.drop(409)

df[df['LoanAmount'] > 400 ].sort_values(by = 'ApplicantIncome')

"""Only 4 out of 15 of the loans were denied, but since many of the incomes are fairly high this doesn't seem completely unreasonable.

While some of these points could be questioned, we lack significant evidence that any of these points should be removed.
"""

print(df.groupby('Loan_Status')['ApplicantIncome'].mean())
df.groupby('Loan_Status')['ApplicantIncome'].mean().plot.bar()
P = plt.ylabel('mean applicant income')
plt.show()

# making bins for applicant income variable
bins = [0,2500,4000,6000,81000]
group = ['Low','Average','High', 'Very high']
df['Income_bin'] = pd.cut(df['ApplicantIncome'],bins,labels=group)
df.head()

print(pd.crosstab(df['Income_bin'],df['Loan_Status']))
Income_bin = pd.crosstab(df['Income_bin'],df['Loan_Status'])
Income_bin.div(Income_bin.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('ApplicantIncome')
P = plt.ylabel('Percentage')
plt.show()

"""Applicant income does not affect the chances of loan approval which contradicts our hypothesis in which we assumed that if the applicant income is high the chances of loan approval will also be high"""

# making bins for Coapplicant income variable
bins = [0,1000,3000,42000]
group = ['Low','Average','High']
df['Coapplicant_Income_bin'] = pd.cut(df['CoapplicantIncome'],bins,labels=group)

# plot the chart
Coapplicant_Income_bin = pd.crosstab(df['Coapplicant_Income_bin'],df['Loan_Status'])
Coapplicant_Income_bin.div(Coapplicant_Income_bin.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('CoapplicantIncome')
P = plt.ylabel('Percentage')
plt.show()

print(len(df[df["CoapplicantIncome"] == 0]))
"Percentage of CoapplicantIncome = 0 is:",  len(df[df["CoapplicantIncome"] == 0])/len(df["CoapplicantIncome"])

"""- Historically if coapplicant’s income is less the chances of loan approval are high. 
- However, most of the applicants don’t have any coapplicant, so the coapplicant income for such applicants is 0 and hence the loan approval is not dependent on it. 
- So we can make a new variable in which we will combine the applicant’s and coapplicant’s income 
"""

# create a new variable
df['Total_Income'] = df['ApplicantIncome'] + df['CoapplicantIncome']

# making bins for Total Income variable
bins = [0,2500,4000,6000,81000]
group = ['Low','Average','High', 'Very high']
df['Total_Income_bin'] = pd.cut(df['Total_Income'],bins,labels=group)

# plot the chart
Total_Income_bin = pd.crosstab(df['Total_Income_bin'],df['Loan_Status'])
Total_Income_bin.div(Total_Income_bin.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('Total_Income')
P = plt.ylabel('Percentage')
plt.show()

"""- Proportion of loans getting approved for applicants having low Total_Income is very less as compared to that of applicants with Average, High and Very High Income. 
- This is more consistent with our hypothesis with applicants with high income will have more chances of loan approval.
"""

# making bins for LoanAmount variable
bins = [0,100,200,700]
group = ['Low','Average','High']
df['LoanAmount_bin'] = pd.cut(df['LoanAmount'],bins,labels=group)

# plot the chart 
LoanAmount_bin = pd.crosstab(df['LoanAmount_bin'],df['Loan_Status'])
LoanAmount_bin.div(LoanAmount_bin.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('LoanAmount')
P = plt.ylabel('Percentage')
plt.show()

"""- proportion of approved loans is higher for Low and Average Loan Amount as compared to that of High Loan Amount which supports our hypothesis in which we considered that the chances of loan approval will be high when the loan amount is less."""

df.head()

# drop the new variable of bins
df.drop(columns= ['Income_bin', 'Coapplicant_Income_bin', 'LoanAmount_bin', 'Total_Income_bin', 'Total_Income'], axis=1, inplace=True)
df.head()

df.Dependents.value_counts()

# replacing 3+ in Dependents variable with 3 for both train and test set
df['Dependents'].replace('3+', 3, inplace=True)
df.Dependents.value_counts()

# replacing Y and N in Loan_Status variable with 1 and 0 respectively
df['Loan_Status'].replace('N', 0, inplace=True)
df['Loan_Status'].replace('Y', 1, inplace=True)
df.head()

# calculate and visualize correlation matrix
matrix = df.corr()
f, ax = plt.subplots(figsize=(10, 6))
sns.heatmap(matrix, vmax=1, square=True, cmap="BuPu", annot=True)
matrix

"""## Multi-variate analysis"""

i, j = 0, 0
f, axes = plt.subplots(6, 2, figsize = (11, 20))
plt.subplots_adjust(hspace = .45)

for col in df.columns[1:]:
    if col in ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']:
        axes[i,j].hist(df[col].dropna())
    else:
        bar_data = df[col].value_counts()
        sns.barplot(bar_data.index, bar_data.values, ax = axes[i,j])
    axes[i, j].set_title(col)
    axes[i, j].spines['top'].set_visible(False)
    axes[i, j].spines['right'].set_visible(False)
    j += 1 
    if j ==2:
        i += 1
        j = 0

"""less than half of our loans classified as not accepted, therefore perform stratefied cv would be a better approach.

# Data Pre-processing

### Missing value and outlier treatment

"""

#report number of missing values for each feature
for col in df.columns:
    missing_series = df[col][df[col].isna() == True]
    if missing_series.size > 0:
        print(col, missing_series.size)
    plt.show()

"""- numerical variables: imputation using mean or median
- categorical variables: imputation using mode
- There are very less missing values in Gender, Married, Dependents, Credit_History and Self_Employed features so we fill them using the mode of the features. 
- If an independent variable in our dataset has huge amount of missing data e.g. 80% missing values in it, then we would drop the variable from the dataset.
"""

#fill missing categorical values with mode
categ_cols = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History']
for col in categ_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

df['Loan_Amount_Term'].value_counts()
# value of 360 is repeating the most. 
# So we will replace the missing values in this variable using the mode of this variable.

# replace missing value with the mode
df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)
df['Loan_Amount_Term'].value_counts()

# replace missing values with the median value due to outliers
df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)
df.isnull().sum()

"""# Loading test set"""

uploaded = files.upload()

# Import data 
df1 = pd.read_csv("loan_test.csv")
df1.info()

# replacing 3+ in Dependents variable with 3 for both train and test set
df1['Dependents'].replace('3+', 3, inplace=True)

# replace missing values in Test set with mode/median from Training set
df1['Gender'].fillna(df['Gender'].mode()[0], inplace=True)
df1['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)
df1['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)
df1['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)
df1['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)
df1['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)

# check whether all the missing values are filled in the Test dataset
df1.isnull().sum()

"""### Outlier Treatment"""

# original data

ax1 = plt.subplot(121)
df['LoanAmount'].hist(bins=20, figsize=(10,4))
ax1.set_title("Train")

ax2 = plt.subplot(122)
df1['LoanAmount'].hist(bins=20)
ax2.set_title("Test")
plt.show()

# Removing skewness in LoanAmount variable by log transformation
df['LoanAmount_log'] = np.log(df['LoanAmount'])
df1['LoanAmount_log'] = np.log(df1['LoanAmount'])

# after log transformation

ax1 = plt.subplot(121)
df['LoanAmount_log'].hist(bins=20, figsize=(10,4))
ax1.set_title("Train")

ax2 = plt.subplot(122)
df1['LoanAmount_log'].hist(bins=20)
ax2.set_title("Test")
plt.show()